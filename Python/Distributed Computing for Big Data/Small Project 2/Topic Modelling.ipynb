{"cells": [{"cell_type": "markdown", "metadata": {"id": "I2MAGZ2L7NJj"}, "source": "# ST446 Distributed Computing for Big Data\n## Assignment 2 - PART 2\n---\n\n## P2: Topic Modelling\n\nIn this homework problem, you are asked to perform a semantic analysis of the DBLP author publications dataset `dblp/author_large.txt`.\n\nPlease, refer to:\n* Week02 on how to download this dataset into the master node of your Dataproc cluster. You can put this dataset into a bucket on your GCP account and access it from your code as well.\n* Week09 on how to configure your Dataproc cluster to use the `NLTK` library.\n* Week09 on an example code for running LDA topic modelling.\n\n## Questions\n\n**P2.A (25 points)** Use Latent Dirichlet Allocation (LDA) to cluster publications by using words in their titles and represent each publication by 10 topics. Please follow these steps:\n\n**A.1** Convert titles to tokens by:\n   * Tokenizing words in the title of each publication.\n   * Removing stop words using the `nltk` package.\n   * Removing puctuations, numbers or other symbols.\n   * Lemmatizing tokens.\n\nNote that you may skip some of these editing steps or add some additional steps to edit the tokens, but if you do this provide a justification for it.\n\n**A.2** Convert tokens into sparse vectors.\n\n**A.3** Use LDA to find out 10 topics for each publication and represent each topic with the first few most relevant words. Note that you can choose to use different number of topics rather than 10. Again if you do so, please provide a justification.\n\n**A.4** Comment the obtained results.\n\n**P2.B (25 points)** Address each question as in part A, but with each *document* representing all publication tiles of a specific author. For example, if an author $Y$ wrote \"introduction to databases\" and \"database design\", then the *document* for the author $Y$ will be \"introduction to database database design\". \n\nIn addition, calculate the **topic density** vector for each author and use the topic density to calculate the **cosine similarity** for each pair of authors. For example, if the topic density for author X is $[x_1, x_2, x_3, \\dots]$ and topic density vector for author Y is $[y_1, y_2, y_3, \\dots]$, then the cosine similarity is $\\frac{x_1\\cdot y_1 + x_2\\cdot y_2 + x_3\\cdot y_3 +\\dots}{\\sqrt{x_1^2+ x_2^2+ x_3^2 +\\dots}\\sqrt{y_1^2+ y_2^2+ y_3^2 +\\dots}}$. Show the 10 most similar author pairs and comment on their similarity, if possible taking into consideration the results from the previous section."}, {"cell_type": "markdown", "metadata": {"id": "EJa1YyUG7NJl"}, "source": "## 0. Load data"}, {"cell_type": "code", "execution_count": 95, "metadata": {"id": "KfcsGHg_7NJm", "scrolled": true}, "outputs": [], "source": "import numpy as np\n\n# your code to adjust the path to your dataset author-large.txt\nauthor_rdd2 = sc.textFile('gs://bucket-sar/author-large.txt', 4) \\\n                .map(lambda row: np.array(row.strip().split(\"\\t\")))"}, {"cell_type": "code", "execution_count": 96, "metadata": {}, "outputs": [], "source": "#I take a sample of the dataset of 707 records, because the full dataset takes a really long time to run:\nauthor_rdd = author_rdd2.filter(lambda r :(int(r[3])== 2010 and 'al' in r[0] ) )"}, {"cell_type": "code", "execution_count": 97, "metadata": {"scrolled": true}, "outputs": [{"data": {"text/plain": "707"}, "execution_count": 97, "metadata": {}, "output_type": "execute_result"}], "source": "author_rdd.count()"}, {"cell_type": "code", "execution_count": 98, "metadata": {"id": "ZtDwN5BV7NJm"}, "outputs": [], "source": "# example on how you can manipulate the RDD containing the data\n# you can adjust for your case\n\n#authors = author_rdd.map(lambda r: (r[0],1)).reduceByKey(lambda a,b: a+b)\n#author_30 = set(authors.filter(lambda r: r[1] >= 30).map(lambda r: r[0]).collect())"}, {"cell_type": "code", "execution_count": 99, "metadata": {"id": "V4jrVTZm7NJn", "outputId": "14604497-ec76-4419-d9e7-7edf51e417e3"}, "outputs": [], "source": "#title_author = author_rdd.filter(lambda r: r[0] in author_30). \\\n#                    map(lambda r: (r[0],r[2])).distinct()\n#title_author.take(10)"}, {"cell_type": "code", "execution_count": 100, "metadata": {"id": "97acIVei7NJo", "outputId": "aeecc32d-4371-4cde-9b13-0ec4079d9d01"}, "outputs": [], "source": "#print(author_rdd.count())\n#print(title_author.count())"}, {"cell_type": "code", "execution_count": 101, "metadata": {}, "outputs": [], "source": "#Preparation for part A:\n#Get all the titles in one object, so that they appear only once.\ntitles = author_rdd.map(lambda r: (r[2])).distinct()"}, {"cell_type": "code", "execution_count": 102, "metadata": {}, "outputs": [], "source": "#Preparation for part B:\n#Get all publication tiles of a specific author in one row, so that each document represents all the publications of an author:\nauthor_publ = author_rdd.map(lambda r: (r[0],r[2])).distinct()\nauthor_publ = author_publ.reduceByKey(lambda x, y: x + ' ' + y)\nauthor_publ = author_publ.map(lambda r: (r[0],r[1]))"}, {"cell_type": "markdown", "metadata": {"id": "qHwbHeY77NJo"}, "source": "## A1. Parse the data\n\nHere we make use of the natural language processing module `nltk`. Please download both the module and the corresponding data. See https://www.nltk.org/install.html and https://www.nltk.org/data.html for more details."}, {"cell_type": "code", "execution_count": 103, "metadata": {"id": "6M_qGmj-7NJp"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[('Dual-Level Attack Detection and Characterization for Networks under DDoS.', ['duallevel', 'attack', 'detection', 'characterization', 'network', 'ddos'])]\n"}, {"data": {"text/plain": "['attack', 'detection', 'network']"}, "execution_count": 103, "metadata": {}, "output_type": "execute_result"}], "source": "from nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\n\n#A.1 Convert titles to tokens by:\n#Tokenizing words in the title of each publication.\n#Removing stop words using the nltk package.\n#Removing puctuations, numbers or other symbols.\n#Lemmatizing tokens.\n\nstop_words = set(stopwords.words('english'))\ntable = str.maketrans('', '', string.punctuation)\nlmtzr = WordNetLemmatizer() \n\ndef get_tokens(line):\n    # get tokens from line\n    tokens = word_tokenize(line)\n    # convert to lower case\n    tokens = [w.lower() for w in tokens]\n    # remove punctuations from each word\n    stripped = [w.translate(table) for w in tokens]\n    # remove remaining tokens that are not alphabetic\n    words = [word for word in stripped if word.isalpha()]\n    # filter out stop words\n    words = [w for w in words if not w in stop_words]\n    # lemmatizing the words\n    words = [lmtzr.lemmatize(w) for w in words]\n    return (words)\n\ntitles_tokens = titles.map(lambda line: (str(line), get_tokens(line)))\nprint(titles_tokens.take(1))\n\n\n#Remove stop words based on document features:\n\n'''Find and store words that appear extremely rarely in the documents. Here, we consider these words to appear\nless than 2 times across all titles. These words will be removed, hence they will not become features. \nThe reason is that words that appear so rarely cannot provide a guideline on the topic, \nbut might only overfit and overcomplicate the model. If the code would run in the full datset I would select \na higher threshold than 2 for identifying rare words and removing them.\n'''\ndoc_stop_words = titles_tokens.flatMap(lambda r: r[1]).map(lambda r: (r,1)).reduceByKey(lambda a,b: a+b)\ndoc_stop_words = doc_stop_words.filter(lambda a: a[1]<2).map(lambda r: r[0]).collect()\n\n# throw away stop words and words that are just single letters.\ntitles_nostop = titles_tokens.map(lambda r: (r[0],[w for w in r[1] if not w in doc_stop_words and not len(w)==1])) \n\ntitles_nostop.take(1)[0][1][:10]"}, {"cell_type": "markdown", "metadata": {"id": "B4kE5SGl7NJp"}, "source": "## A2. Convert tokens into sparse vectors\n"}, {"cell_type": "code", "execution_count": 104, "metadata": {"id": "sSN7zYvT7NJp", "outputId": "01a49e17-f0e1-4d8c-8db1-9132b443e795"}, "outputs": [{"data": {"text/plain": "[Row(title='Dual-Level Attack Detection and Characterization for Networks under DDoS.', words=['attack', 'detection', 'network']),\n Row(title='A Probabilistic Approach for On-Line Sum-Auditing.', words=['probabilistic', 'approach', 'online'])]"}, "execution_count": 104, "metadata": {}, "output_type": "execute_result"}], "source": "# your code\nfrom pyspark.ml.feature import CountVectorizer\nfrom pyspark.sql.functions import monotonically_increasing_id\n\ntitles_df = spark.createDataFrame(titles_nostop, [\"title\",\"words\"])\ntitles_df.cache()\ntitles_df.take(2)"}, {"cell_type": "markdown", "metadata": {"id": "xjP2RHegERud"}, "source": "## Generate a vectorized representation of the *tokens*\n"}, {"cell_type": "code", "execution_count": 105, "metadata": {"id": "JRHdUsAx7NJq", "outputId": "2c9d60e4-b7b9-4500-9009-4eee5e2a76b7"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+--------------------+\n|               title|               words|            features|\n+--------------------+--------------------+--------------------+\n|Dual-Level Attack...|[attack, detectio...|(325,[5,25,97],[1...|\n|A Probabilistic A...|[probabilistic, a...|(325,[4,56],[1.0,...|\n|Analysing and Vis...|[security, usabil...|    (325,[45],[1.0])|\n+--------------------+--------------------+--------------------+\nonly showing top 3 rows\n\n"}], "source": "# your code\n\n# convert a collection of text documents to a matrix of token counts.\n''' `minDF`: I use the value of 3, so that words that appear in less than 3 documents \n#won't be part of the vectorized representation of tokens. The reason is that is they are so rare, \n#then it will be like overfitting the model later on.'''\ncv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", minDF=3)\n\n# learn a vocabulary dictionary of all tokens in the raw documents.\ncv_model = cv.fit(titles_df)\n\n# learn the vocabulary dictionary and return document-term matrix.\ntitles_df_w_features = cv_model.transform(titles_df)\ntitles_df_w_features.cache()\ntitles_df_w_features.show(3)"}, {"cell_type": "markdown", "metadata": {"id": "UxS38Bsj7NJq"}, "source": "## Convert pyspark.ml vectors to pyspark.mllib vectors"}, {"cell_type": "code", "execution_count": 106, "metadata": {"id": "mhWffk1r7NJq"}, "outputs": [{"data": {"text/plain": "[SparseVector(325, {5: 1.0, 25: 1.0, 97: 1.0})]"}, "execution_count": 106, "metadata": {}, "output_type": "execute_result"}], "source": "# your code\nfrom pyspark.mllib.linalg import Vectors\n\ndef as_mllib_vector(v):\n    return Vectors.sparse(v.size, v.indices, v.values)\n\nfeatures = titles_df_w_features.select(\"features\")\nfeature_vec = features.rdd.map(lambda r: as_mllib_vector(r[0]))\n\nfeature_vec.cache()\nfeature_vec.take(1)"}, {"cell_type": "markdown", "metadata": {"id": "X0JxSajd7NJr"}, "source": "## Check the vocabulary"}, {"cell_type": "code", "execution_count": 107, "metadata": {"id": "ROccPfR27NJr", "outputId": "d95bcfbd-c4cd-4f35-b90b-b4f19b68956d"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Vocabulary from CountVectorizerModel is:\n['de', 'system', 'model', 'using', 'approach', 'network', 'application', 'analysis', 'performance', 'information', 'service', 'data', 'social', 'framework', 'based', 'computing', 'towards', 'learning', 'efficient', 'memory', 'environment', 'design', 'study', 'tool', 'program', 'detection', 'computer', 'pour', 'development', 'programming', 'problem', 'science', 'software', 'distributed', 'management', 'la', 'virtual', 'architecture', 'code', 'mobile', 'algorithm', 'user', 'language', 'support', 'pattern', 'security', 'dynamic', 'classification', 'wireless', 'knowledge', 'high', 'interaction', 'interface', 'parallel', 'multiple', 'automatic', 'online', 'value', 'logic', 'semantic', 'sensor', 'transformation', 'simulation', 'structure', 'technology', 'search', 'education', 'base', 'web', 'modeling', 'role', 'access', 'optimization', 'challenge', 'community', 'der', 'abstract', 'java', 'video', 'interactive', 'experience', 'evaluation', 'le', 'local', 'group', 'case', 'hybrid', 'clustering', 'document', 'digital', 'control', 'et', 'semantics', 'segmentation', 'computational', 'building', 'strategy', 'attack', 'sur', 'infrastructure']\n\n---\n\nNumber of terms m:  325\n"}], "source": "# your code\n\nprint (\"Vocabulary from CountVectorizerModel is:\")\nprint(cv_model.vocabulary[:100])\nprint(\"\\n---\\n\")\n\nm = len(cv_model.vocabulary)\nprint(\"Number of terms m: \", m)"}, {"cell_type": "markdown", "metadata": {"id": "JHmr73aL7NJr"}, "source": "## A3. Latent Dirichlet Allocation"}, {"cell_type": "code", "execution_count": 108, "metadata": {"id": "z-D8pLq07NJr", "scrolled": true}, "outputs": [], "source": "# your code\nfrom pyspark.ml.clustering import LDA\n\n# instantiate LDA model. It is a batch LDA using EM algorithm. \n#Even though the online LDA can be more accurate, we use batch, as it executes quicker.\n#I select 10 topics, which could fit better in the full dataset.\nlda = LDA(k=10, maxIter=5)\n# training the model\nlda_model = lda.fit(titles_df_w_features)\n\n#LONG TIME:\n# calculate logLikelihood\n#ll = lda_model.logLikelihood(titles_df_w_features)\n# calculate perplexity\n#lp = lda_model.logPerplexity(titles_df_w_features)\n\n#print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n#print(\"The upper bound on the perplexity: \" + str(lp))"}, {"cell_type": "markdown", "metadata": {"id": "l9roli0F7NJs"}, "source": "Looking at the topics:"}, {"cell_type": "code", "execution_count": 109, "metadata": {"id": "k9VM2LMi7NJs", "outputId": "1466efd5-c7bd-40db-e3dd-97f91392b4d5", "scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "The topics described by their top-weighted terms:\n+-----+------------------------+----------------------------------------------------------------------------------------------------------------+\n|topic|termIndices             |termWeights                                                                                                     |\n+-----+------------------------+----------------------------------------------------------------------------------------------------------------+\n|0    |[1, 134, 59, 9, 170]    |[0.02084882858555773, 0.00777319155846653, 0.007376292704212854, 0.007321170771130758, 0.00656767314316723]     |\n|1    |[51, 214, 240, 104, 22] |[0.008452264510354676, 0.007230752479272849, 0.006926990275135945, 0.005610810024368863, 0.0053655463460259255] |\n|2    |[31, 26, 66, 11, 44]    |[0.010973444595536574, 0.00925387455292773, 0.008174444940824591, 0.007856212059772893, 0.0073779446604198985]  |\n|3    |[191, 285, 309, 176, 72]|[0.0105412676897365, 0.008483363048266844, 0.00837983671272462, 0.008341795035924846, 0.006522442037528656]     |\n|4    |[291, 149, 63, 129, 163]|[0.006570942665427559, 0.005485324900040956, 0.005337821967700949, 0.00532595836866162, 0.005272888236720771]   |\n|5    |[128, 24, 4, 19, 28]    |[0.008543437815198856, 0.008345434421132891, 0.008286840944806181, 0.008207198106598115, 0.00788152229108573]   |\n|6    |[22, 259, 63, 211, 280] |[0.008358072978473243, 0.005342282025102091, 0.005319472642361062, 0.0052173908895028175, 0.005130578400139755] |\n|7    |[151, 55, 93, 162, 69]  |[0.0052043723535009955, 0.005188146111394603, 0.005151692044162321, 0.005130831013131392, 0.0051272280626491655]|\n|8    |[0, 2, 27, 35, 10]      |[0.018106614237521492, 0.015203516094919866, 0.011868271495375724, 0.010306865382067824, 0.008688165740013807]  |\n|9    |[9, 46, 12, 7, 76]      |[0.013728612682793863, 0.010987065049202329, 0.010632394845060355, 0.0089553229676698, 0.008929520339555335]    |\n+-----+------------------------+----------------------------------------------------------------------------------------------------------------+\n\n"}], "source": "# Describe topics\n# your code\n\ntopics = lda_model.describeTopics(5)\n\nprint(\"The topics described by their top-weighted terms:\")\n\ntopics.show(truncate=False)"}, {"cell_type": "code", "execution_count": 110, "metadata": {"id": "By2qFduy7NJs", "outputId": "a9ec1662-3ca5-4258-e429-7e79bdf494a2", "scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "topic 1 : ['system' 'functional' 'semantic' 'information' 'multiagent']\ntopic 2 : ['interaction' 'tree' 'assignment' 'game' 'study']\ntopic 3 : ['science' 'computer' 'education' 'data' 'pattern']\ntopic 4 : ['solution' 'ehealth' 'emerging' 'enhancing' 'optimization']\ntopic 5 : ['algebraic' 'global' 'structure' 'team' 'medium']\ntopic 6 : ['transactional' 'program' 'approach' 'memory' 'development']\ntopic 7 : ['study' 'feasibility' 'structure' 'par' 'hardware']\ntopic 8 : ['integration' 'automatic' 'segmentation' 'partial' 'modeling']\ntopic 9 : ['de' 'model' 'pour' 'la' 'service']\ntopic 10 : ['information' 'dynamic' 'social' 'analysis' 'abstract']\n"}], "source": "# Shows the results\n# your code\ntopic_i = topics.select(\"termIndices\").rdd.map(lambda r: r[0]).collect()\nfor j, i in enumerate(topic_i):\n    print('topic', j+1, ':', np.array(cv_model.vocabulary)[i])"}, {"cell_type": "markdown", "metadata": {"id": "z5jJOFTKGWSe"}, "source": "## A4. Comment your results"}, {"cell_type": "markdown", "metadata": {}, "source": "From the results abe we see that 10 topics have been created for the titles and we see the words that mainly describe thse topics. Topic 1 has a higher presence in the document (title) when the words 'algorithm', 'challenge','using', 'efficient' and 'student' occur inhigh percentage. e see that most if the topics are related to computer science, but this might be due to the small dataset used, since only 325 tokens were used in the model."}, {"cell_type": "markdown", "metadata": {"id": "0fIECxa_7NJs"}, "source": "## B1. Convert tokens into sparse vectors"}, {"cell_type": "code", "execution_count": 121, "metadata": {"id": "wQxKrPsD7NJt", "scrolled": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[('Ana Cavalcanti', ['communication', 'system', 'clawz'])]\n"}, {"data": {"text/plain": "['suivi', 'dautomobiles', 'par', 'classification', 'hirarchique']"}, "execution_count": 121, "metadata": {}, "output_type": "execute_result"}], "source": "# your code\n#B Convert titles to tokens by:\n#Tokenizing words in the title of each publication.\n#Removing stop words using the nltk package.\n#Removing puctuations, numbers or other symbols.\n#Lemmatizing tokens.\n\nstop_words = set(stopwords.words('english'))\ntable = str.maketrans('', '', string.punctuation)\nlmtzr = WordNetLemmatizer() # see https://www.nltk.org/_modules/nltk/stem/wordnet.html for details\n\nauthor_publ_toks = author_publ.map(lambda line: (str(line[0]), get_tokens(line[1])))\nprint(author_publ_toks.take(1))\n\n\n#Remove stop words based on document features:\n\n'''Find and store words that appear extremely rarely in the documents. Here, we consider these words to appear\nless than 2 times across all concatinated titles of authors. These words will be removed, hence they will not become features. \nThe reason is that words that appear so rarely cannot provide a guideline on the topic, \nbut might only overfit and overcomplicate the model. If the code would run in the full dataset I would select \na higher threshold than 2 for identifying rare words and removing them.\n'''\n\ndoc_stop_words2 = author_publ_toks.flatMap(lambda r: r[1]).map(lambda r: (r,1)).reduceByKey(lambda a,b: a+b)\ndoc_stop_words2 = doc_stop_words2.filter(lambda a: a[1]<2).map(lambda r: r[0]).collect()\n\n# throw away stop words and words that are just single letters.\nauthor_publ_nostop = author_publ_toks.map(lambda r: (r[0],[w for w in r[1] if not w in doc_stop_words2 and not len(w)==1])) \n\nauthor_publ_nostop.take(1)[0][1][:10]\n"}, {"cell_type": "code", "execution_count": 122, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(author='Ana Cavalcanti', words=['communication', 'system']),\n Row(author='Sudhakar Yalamanchili', words=['modeling', 'workload', 'system'])]"}, "execution_count": 122, "metadata": {}, "output_type": "execute_result"}], "source": "#Convert tokens into sparse vectors\nauthor_publ_df = spark.createDataFrame(author_publ_nostop, [\"author\",\"words\"])\nauthor_publ_df.cache()\nauthor_publ_df.take(2)"}, {"cell_type": "markdown", "metadata": {"id": "1VkqEpsAHfBC"}, "source": "## Generate a vectorized representation of the *tokens*"}, {"cell_type": "code", "execution_count": 123, "metadata": {"id": "8bs-UsHT7NJu", "outputId": "e9d54251-c37d-4b6c-bfc0-81f48a29d45d", "scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------------+----------------------------+-------------------------------+\n|author               |words                       |features                       |\n+---------------------+----------------------------+-------------------------------+\n|Ana Cavalcanti       |[communication, system]     |(885,[1,125],[1.0,1.0])        |\n|Sudhakar Yalamanchili|[modeling, workload, system]|(885,[1,102,425],[1.0,1.0,1.0])|\n+---------------------+----------------------------+-------------------------------+\nonly showing top 2 rows\n\n"}], "source": "# your code\n# convert a collection of text documents to a matrix of token counts.\n''' `minDF`: We dont use a minDF because here the documents are way less and longer than the title-documents we used before. \nHence in this small dataset an author might be writing about a topic that none else will.'''\ncv2 = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n\n# learn a vocabulary dictionary of all tokens in the raw documents.\ncv_model2 = cv2.fit(author_publ_df)\n\n# learn the vocabulary dictionary and return document-term matrix.\nauthor_publ_df_w_features = cv_model2.transform(author_publ_df)\nauthor_publ_df_w_features.cache()\nauthor_publ_df_w_features.show(2, truncate = False)"}, {"cell_type": "markdown", "metadata": {"id": "UdVwBNdt7NJu"}, "source": "## Convert pyspark.ml vectors to pyspark.mllib vectors"}, {"cell_type": "code", "execution_count": 124, "metadata": {"id": "6MpyTa5q7NJv", "outputId": "c211e3a6-9049-49fc-9d7f-00d674f4b7f8"}, "outputs": [{"data": {"text/plain": "[SparseVector(885, {1: 1.0, 125: 1.0})]"}, "execution_count": 124, "metadata": {}, "output_type": "execute_result"}], "source": "# your code\n\nfeatures2 = author_publ_df_w_features.select(\"features\")\nfeature_vec2 = features2.rdd.map(lambda r: as_mllib_vector(r[0]))\n\n#feature_vec2.cache()\nfeature_vec2.take(1)"}, {"cell_type": "markdown", "metadata": {"id": "CatbxUVB7NJv"}, "source": "### Take a look at the vocabulary"}, {"cell_type": "code", "execution_count": 125, "metadata": {"id": "cgSlC5nT7NJv", "outputId": "a269f005-d5c1-4267-ef90-90583853c984"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Vocabulary from CountVectorizerModel is:\n['de', 'system', 'model', 'using', 'approach', 'network', 'analysis', 'application', 'service', 'data', 'performance', 'information', 'efficient', 'computing', 'memory', 'social', 'development', 'program', 'based', 'study', 'framework', 'environment', 'design', 'pour', 'towards', 'tool', 'learning', 'software', 'management', 'programming', 'science', 'computer', 'detection', 'architecture', 'support', 'distributed', 'dynamic', 'interaction', 'interface', 'security', 'classification', 'technology', 'pattern', 'user', 'virtual', 'multiple', 'problem', 'mobile', 'algorithm', 'la', 'code', 'base', 'wireless', 'language', 'parallel', 'sensor', 'knowledge', 'education', 'health', 'document', 'logic', 'structure', 'automatic', 'high', 'optimization', 'planning', 'transactional', 'semantic', 'transformation', 'search', 'web', 'interactive', 'et', 'simulation', 'strategy', 'experience', 'le', 'role', 'patient', 'control', 'semantics', 'value', 'java', 'monitoring', 'une', 'evaluation', 'infrastructure', 'process', 'challenge', 'hybrid', 'clustering', 'business', 'video', 'online', 'access', 'platform', 'solution', 'der', 'scalable', 'community']\n\n---\n\nNumber of terms m:  885\n"}], "source": "# your code\nprint (\"Vocabulary from CountVectorizerModel is:\")\nprint(cv_model2.vocabulary[:100])\nprint(\"\\n---\\n\")\n\nm = len(cv_model2.vocabulary)\nprint(\"Number of terms m: \", m)"}, {"cell_type": "markdown", "metadata": {"id": "dWAqVT027NJw"}, "source": "## B1. Latent Dirichlet Allocation\n\nWe now analyse the same dataset but using the Latent Dirichlet Allocation to find feature vectors characterizing topics of documents, and feature vectors characterizing the words of topics.\n\n"}, {"cell_type": "code", "execution_count": 126, "metadata": {"id": "3F4twedy7NJw"}, "outputs": [], "source": "# your code\n# instantiate LDA model\nlda2 = LDA(k=10, maxIter=5).setTopicDistributionCol(\"topicDistributionCol\")\n# training the model\nlda_model2 = lda2.fit(author_publ_df_w_features)\n\n#LONG TIME:\n# calculate logLikelihood\n#ll2 = lda_model2.logLikelihood(author_publ_df_w_features)\n# calculate perplexity\n#lp2 = lda_model2.logPerplexity(author_publ_df_w_features)\n\n#print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n#print(\"The upper bound on the perplexity: \" + str(lp))"}, {"cell_type": "markdown", "metadata": {"id": "AitucvvP7NJw"}, "source": "The perplexity below is a measurement of how well a probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample."}, {"cell_type": "code", "execution_count": 127, "metadata": {"id": "A4HCsUrS7NJx", "outputId": "b6e4db34-1c68-418b-bbf6-0ec64f63fcee", "scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "The topics described by their top-weighted terms:\n+-----+-------------------------+------------------------------------------------------------------------------------------------------------------+\n|topic|termIndices              |termWeights                                                                                                       |\n+-----+-------------------------+------------------------------------------------------------------------------------------------------------------+\n|0    |[11, 128, 20, 1, 68]     |[0.0038467355846325746, 0.0038105359724533434, 0.0037395917580262776, 0.0037356877350951446, 0.003630661180328617]|\n|1    |[5, 83, 373, 643, 55]    |[0.003088524103112293, 0.003023962986769739, 0.002778274468819081, 0.002582373221437107, 0.002576776369402407]    |\n|2    |[1, 3, 48, 10, 27]       |[0.008133595017822766, 0.005726834451699499, 0.005083177207777827, 0.004925496432479812, 0.003956571874342173]    |\n|3    |[559, 117, 560, 38, 862] |[0.0020747157667571144, 0.0020732698553482644, 0.002023988978227356, 0.002009902135442102, 0.00198499418321122]   |\n|4    |[6, 477, 648, 32, 162]   |[0.0031061491695244655, 0.002639626596394878, 0.002536375935640124, 0.002469619278812229, 0.0023755509060740445]  |\n|5    |[7, 34, 287, 22, 162]    |[0.003820892236614102, 0.0036145160590332027, 0.003212402213817684, 0.0031658907325878807, 0.0029753104197101574] |\n|6    |[29, 829, 101, 157, 141] |[0.003128845058892722, 0.0025928307924692477, 0.002515662859187691, 0.0024754138244522873, 0.0023484447150770547] |\n|7    |[576, 415, 446, 786, 529]|[0.001911697683265599, 0.0019062062050274216, 0.0018382673605003793, 0.0018077815639520136, 0.0017996134132362352]|\n|8    |[0, 23, 97, 92, 4]       |[0.009163629283145366, 0.004351746058422428, 0.004077030871759633, 0.003783386244235678, 0.0037642759722475313]   |\n|9    |[828, 205, 802, 41, 637] |[0.002031259578322305, 0.0019888574343539557, 0.0019720037676338063, 0.001928214921237416, 0.0019224401249026827] |\n+-----+-------------------------+------------------------------------------------------------------------------------------------------------------+\n\n"}], "source": "# Describe topics and top-weighted terms\n# your code\ntopics2 = lda_model2.describeTopics(5)\n\nprint(\"The topics described by their top-weighted terms:\")\n\ntopics2.show(truncate=False)"}, {"cell_type": "code", "execution_count": 128, "metadata": {"id": "8FQh8Slh7NJx", "outputId": "ea88596f-32ac-4d95-d46c-ea414ec0a5b4"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "topic 1 : ['information' 'extended' 'framework' 'system' 'transformation']\ntopic 2 : ['network' 'monitoring' 'comparing' 'combinatorial' 'sensor']\ntopic 3 : ['system' 'using' 'algorithm' 'performance' 'software']\ntopic 4 : ['graphic' 'physical' 'profitable' 'interface' 'possibility']\ntopic 5 : ['analysis' 'permutation' 'lowlevel' 'detection' 'array']\ntopic 6 : ['application' 'support' 'mapping' 'design' 'array']\ntopic 7 : ['programming' 'shape' 'developing' 'graph' 'execution']\ntopic 8 : ['efficacy' 'call' 'threadlevel' 'speculation' 'graphlevel']\ntopic 9 : ['de' 'pour' 'der' 'video' 'approach']\ntopic 10 : ['biomedical' 'standard' 'applicability' 'technology' 'microassembly']\n"}], "source": "# Shows the results\n# your code\ntopic_i2 = topics2.select(\"termIndices\").rdd.map(lambda r: r[0]).collect()\nfor j, i in enumerate(topic_i2):\n    print('topic', j+1, ':', np.array(cv_model2.vocabulary)[i])"}, {"cell_type": "code", "execution_count": 129, "metadata": {"scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+--------------------+\n|              author|               words|            features|\n+--------------------+--------------------+--------------------+\n|      Ana Cavalcanti|[communication, s...|(885,[1,125],[1.0...|\n|Sudhakar Yalamanc...|[modeling, worklo...|(885,[1,102,425],...|\n+--------------------+--------------------+--------------------+\nonly showing top 2 rows\n\n"}], "source": "cv2 = CountVectorizer(inputCol=\"words\", outputCol=\"features\", minDF = 1) # TO DO 2\n\n# learn a vocabulary dictionary of all tokens in the raw documents.\ncv_model2 = cv2.fit(author_publ_df)\n\n# learn the vocabulary dictionary and return document-term matrix.\nauthor_publ_df_w_features = cv_model2.transform(author_publ_df)\nauthor_publ_df_w_features.cache()\nauthor_publ_df_w_features.show(2, truncate = True)"}, {"cell_type": "markdown", "metadata": {}, "source": "Comment on above results:\n\nHere we have also generated 10 topics, but wee see that some different words have ben picked up. This makes sense because this time we have included 885 tokens in the model. We see for example topic 8 focusing on efficacy and graphlevel, which we didn't see in the top words before."}, {"cell_type": "markdown", "metadata": {"id": "8GIlHFm2IM1R"}, "source": "## B2. Calculate the topic density vector for each author and the cosine similarity ## "}, {"cell_type": "code", "execution_count": 130, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|topicDistributionCol                                                                                                                                                                                          |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|[0.03311606979560238,0.03267691589733875,0.7045669242824184,0.032631219383541736,0.03269794961452489,0.03287849479348196,0.03269988339040325,0.032416311066207765,0.03381987393354242,0.032496357842938535]   |\n|[0.02480202792804722,0.02447107394993546,0.7787396202928778,0.024437429904178687,0.024487869695255633,0.024621943358962596,0.024486896505917746,0.02427608116833271,0.025339893930169367,0.024337163266322733]|\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 2 rows\n\n"}], "source": "#Topic density vector:\ntransformed = lda_model2.transform(author_publ_df_w_features)\ntopics_dens= transformed.select(\"topicDistributionCol\")\ntopics_dens.show(2, truncate= False)\n\n#Transform it from pyspark dataframe into a numpy array:\ntopics_dens_np= np.array(topics_dens.select('topicDistributionCol').collect())\n\n#Transform the lists in the numpy array into arrays:\nncols2 = len(topics_dens_np[0][0])\nnrows2 =len(topics_dens_np)\n\ntopics_dens_np2 = np.zeros((nrows2,ncols2))\n\nfor i in range(nrows2):\n    for j in range(ncols2):\n        topics_dens_np2[i-1, j-1] += topics_dens_np[i-1][0][j-1]"}, {"cell_type": "code", "execution_count": 131, "metadata": {"id": "KhEkCGfm7NJx"}, "outputs": [], "source": "# your code\n\ndef dot_prod(A):\n    B= A.transpose()\n    ncols = len(A[0])\n\n\n    product = np.zeros((nrows,nrows))\n\n    # iterating by row of A\n    for i in range(nrows):\n\n        # iterating by column by B\n        for j in range(nrows):\n\n            # iterating by rows of B\n            for k in range(ncols):\n                product[i-1][j-1] += A[i-1][k-1] * B[k-1][j-1]   \n    return product\n\nnrows= len(topics_dens_np2)\nproduct=dot_prod(topics_dens_np2)"}, {"cell_type": "code", "execution_count": 132, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/opt/conda/anaconda/bin/ipython:21: RuntimeWarning: invalid value encountered in double_scalars\n"}], "source": "# your code\nfrom numpy import array  \nfrom numpy.linalg import norm \n\n\ndef cos_sim(A):\n    \n    #Calculate norms\n    norms = np.zeros(nrows)\n    \n    #cosine similarity\n    cos_sim = np.zeros((nrows,nrows))\n    \n    # iterating by row of A\n    for i in range(nrows):\n        norms[i-1]=norm(A[i-1])\n        \n    for i in range(nrows):\n            # iterating by column \n        for j in range(nrows):\n            cos_sim[i-1][j-1] += product[i-1][j-1]/(norms[i-1]*norms[j-1])  \n        return cos_sim\n    \n\ncos_sim = cos_sim(topics_dens_np2)"}, {"cell_type": "markdown", "metadata": {"id": "YcMjLVY1JXNG"}, "source": "## B3. Show the 10 most similar author pairs and comment on their similarity,"}, {"cell_type": "code", "execution_count": 133, "metadata": {"id": "ELy44cw07NJy", "outputId": "877a4bf8-61c6-4a26-c999-d7732a0e0190"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "1.0000000000000002 [(\"['Stefan Nesbigall']\", \"['Philipp Slusallek']\")]\n0.9999999999687291 [(\"['Stefan Nesbigall']\", \"['Emilio del Rosal Garca']\")]\n0.9999770077981366 [(\"['Stefan Nesbigall']\", \"['Sigurdur O. Adalgeirsson']\"), (\"['Stefan Nesbigall']\", \"['Cynthia Breazeal']\")]\n0.9999769639203687 [(\"['Stefan Nesbigall']\", \"['Maider Zamalloa']\")]\n0.9999769240371887 [(\"['Stefan Nesbigall']\", \"['Aruna D. Balakrishnan']\")]\n0.9999585182482165 [(\"['Stefan Nesbigall']\", \"['Greg Malysa']\")]\n0.9998683270435408 [(\"['Stefan Nesbigall']\", \"['Ananya Kanjilal']\")]\n0.9998681943007313 [(\"['Stefan Nesbigall']\", \"['Gerald DeHondt']\")]\n0.9998681935693876 [(\"['Stefan Nesbigall']\", \"['Marcelo A. Falappa']\")]\n"}], "source": "# your code\n\n#get an array with the name of the authors\nauthor_names= np.array(author_publ_df_w_features.select('author').collect())\n\n#get the ones with the highst correlation:\nfrom collections import defaultdict\n\nd = defaultdict(list)\nfor i in range(len(cos_sim)):\n    for j in range(len(cos_sim)):\n        if i<j:\n            d[cos_sim[i-1][j-1]].append((str(author_names[i-1]),str(author_names[j-1])))\n\nfor value in sorted(d.keys(), reverse=True)[0:9]:\n    print (value, d[value])      "}], "metadata": {"colab": {"collapsed_sections": [], "name": "hw_dblp.ipynb", "provenance": []}, "kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}}, "nbformat": 4, "nbformat_minor": 1}